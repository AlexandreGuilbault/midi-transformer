{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composer_folder = 'Dataset'\n",
    "test_folder = 'Test'\n",
    "output_folder = 'Output'\n",
    "\n",
    "model_tag = 'Fundamental_MidiAI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 3e-4\n",
    "n_epochs = 1000\n",
    "seq_len = 128\n",
    "mini_batch_size = 1024\n",
    "batch_size = 1024\n",
    "\n",
    "mini_batch_cycles = batch_size//mini_batch_size\n",
    "\n",
    "# For max normalization, making sure that target can be a little bit longer than maximum sequence delta\n",
    "stretch = 1.1\n",
    "\n",
    "# GPUs\n",
    "device_ids = [3,2,1,0]\n",
    "device = device_ids[0]\n",
    "\n",
    "batch_display_freq = 1000\n",
    "save_audio_freq = 5000\n",
    "\n",
    "insignificant_velocity= 15\n",
    "insignificant_timestep = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import mido\n",
    "import numpy as np\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, ConcatDataset\n",
    "import torch.nn.functional as f\n",
    "\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_track(track, test=False):\n",
    "\n",
    "    filtered_track = []\n",
    "    \n",
    "    full_delay = 0\n",
    "    for i,msg in enumerate(track):\n",
    "        full_delay += msg.time\n",
    "        if msg.type in ['note_on', 'note_off']:\n",
    "            is_on = (msg.type == 'note_on')\n",
    "            note = msg.note\n",
    "            velocity = msg.velocity\n",
    "\n",
    "            filtered_track.append({'delay':full_delay, \n",
    "                                   'is_on':is_on, \n",
    "                                   'velocity':velocity,\n",
    "                                   'note':note\n",
    "                                   })\n",
    "            full_delay = 0\n",
    "\n",
    "    assert (filtered_track != []), \"(!) Empty Track\"\n",
    "    \n",
    "    encoded_track = []\n",
    "    delay = 0\n",
    "    for i,msg in enumerate(filtered_track):\n",
    "        delay += msg['delay']\n",
    "        if msg['is_on'] and  msg['velocity'] > 0:\n",
    "            length = 0\n",
    "            for j in range(i+1,len(filtered_track)):\n",
    "                length += filtered_track[j]['delay']\n",
    "                if filtered_track[j]['note'] == msg['note']: break\n",
    "            \n",
    "            if delay > 0: encoded_track.append([delay, 0, 128])                \n",
    "            encoded_track.append([length, msg['velocity'], msg['note']])\n",
    "            \n",
    "            delay = 0\n",
    "    \n",
    "    df = pd.DataFrame(encoded_track, columns = [\"Length\", \"Velocity\", \"Note\"]) \n",
    "    \n",
    "    #Remove smallest accidental delays\n",
    "    df.drop(df[(df['Note'] == 128) & (df['Length'] < insignificant_timestep)].index, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def decode_track(encoded_track):\n",
    "\n",
    "    expand_track = []\n",
    "    \n",
    "    track_time = 0\n",
    "    for msg in encoded_track:\n",
    "        \n",
    "        length = max(0,msg[0])\n",
    "        velocity = max(0,min(msg[1],127))\n",
    "        note = max(0,min(msg[2],128))\n",
    "        \n",
    "        if note == 128 : track_time += length\n",
    "        else:\n",
    "            expand_track.append({\n",
    "                'time':track_time, \n",
    "                'is_on':True, \n",
    "                'velocity':velocity,\n",
    "                'note':note})\n",
    "\n",
    "            expand_track.append({\n",
    "                'time':track_time+length, \n",
    "                'is_on':False, \n",
    "                'velocity':velocity,\n",
    "                'note':note\n",
    "                })       \n",
    "    \n",
    "    expand_track.sort(key=lambda msg: msg['time'])\n",
    "    \n",
    "    track_out = mido.MidiTrack()\n",
    "    \n",
    "    previous_time = 0\n",
    "    for msg in expand_track:\n",
    "        time = int(msg['time'])\n",
    "        delta =  time - previous_time\n",
    "        velocity = int(msg['velocity'])\n",
    "        note = int(msg['note'])        \n",
    "        \n",
    "        if msg['is_on']:\n",
    "            track_out.append(mido.Message('note_on', \n",
    "                                           note=note, \n",
    "                                           velocity=velocity, \n",
    "                                           time=delta))\n",
    "        elif not msg['is_on']:\n",
    "            track_out.append(mido.Message('note_off', \n",
    "                                           note=note, \n",
    "                                           velocity=velocity, \n",
    "                                           time=delta))\n",
    "        previous_time = time\n",
    "        \n",
    "    return track_out    \n",
    "\n",
    "def read_midi(path):\n",
    "    if os.path.isdir(path): return None\n",
    "    mid_in = mido.MidiFile(path)\n",
    "    return mid_in\n",
    "\n",
    "def write_midi(mid_out, path):\n",
    "    mid_out.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "# load_path = os.path.join('.','midi_files',composer_folder)\n",
    "\n",
    "# datasets = []\n",
    "# file_name = os.listdir(load_path)[0]\n",
    "# print(file_name)\n",
    "# midi_path = os.path.join(load_path, file_name)\n",
    "# mid_in = read_midi(midi_path)\n",
    "\n",
    "# merged_tracks = mido.merge_tracks(mid_in.tracks)\n",
    "# encoded_track = encode_track(merged_tracks)\n",
    "\n",
    "# track = decode_track(encoded_track.values.tolist())\n",
    "\n",
    "# mid_out = mido.MidiFile(type=0)\n",
    "# mid_out.tracks.append(track)\n",
    "# write_midi(mid_out, './{}.mid'.format(file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(encoded_track, seq_len, test):\n",
    "    \n",
    "    encoded_track = encoded_track.to_numpy()\n",
    "    features_list = []\n",
    "    targets_list = []\n",
    "    for i in range(len(encoded_track)-(seq_len+1)):\n",
    "        features = encoded_track[i:i+seq_len+1,:]        \n",
    "        features_list.append(features)\n",
    "        if test: break\n",
    "    feature_tensors = torch.FloatTensor(np.array(features_list))\n",
    "    dataset = TensorDataset(feature_tensors)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(path, test=False):\n",
    "    \n",
    "    load_path = os.path.join('.','midi_files',path)\n",
    "\n",
    "    datasets = []\n",
    "    files = os.listdir(load_path)\n",
    "    num_files = len(files)\n",
    "    for i, file_name in enumerate(files):\n",
    "        print(\"\\rProgress: {:.2f}%\".format(100 * i/num_files), end=\"\")        \n",
    "        \n",
    "        try:\n",
    "            if (file_name[-4:] == \".mid\") or (file_name[-5:] == \".midi\"):\n",
    "                midi_path = os.path.join(load_path, file_name)\n",
    "                mid_in = read_midi(midi_path)\n",
    "\n",
    "                merged_tracks = mido.merge_tracks(mid_in.tracks)\n",
    "                encoded_track = encode_track(merged_tracks)\n",
    "\n",
    "                datasets.append(create_dataset(encoded_track, seq_len, test))\n",
    "        except:\n",
    "            print(\"\\nIssue with file : {}\".format(file_name))\n",
    "            \n",
    "    return ConcatDataset(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_dataset(composer_folder)\n",
    "test_dataset = get_dataset(test_folder, test=True)\n",
    "\n",
    "data_loader = DataLoader(dataset, shuffle=True, batch_size=mini_batch_size)\n",
    "test_data_loader = DataLoader(test_dataset, shuffle=False, batch_size=mini_batch_size)\n",
    "\n",
    "# data_loader = torch.load('./dataloader.pth')\n",
    "# test_data_loader = torch.load('./test_data_loader.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelativeGlobalAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, max_len=256, dropout=0.1):\n",
    "        super().__init__()\n",
    "        d_head, remainder = divmod(d_model, num_heads)\n",
    "        if remainder:\n",
    "            raise ValueError(\n",
    "                \"incompatible `d_model` and `num_heads`\"\n",
    "            )\n",
    "        self.max_len = max_len\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.key = nn.Linear(d_model, d_model)\n",
    "        self.value = nn.Linear(d_model, d_model)\n",
    "        self.query = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.Er = nn.Parameter(torch.randn(max_len, d_head))\n",
    "        self.register_buffer(\n",
    "            \"mask\", \n",
    "            torch.tril(torch.ones(max_len, max_len))\n",
    "            .unsqueeze(0).unsqueeze(0)\n",
    "        )\n",
    "        # self.mask.shape = (1, 1, max_len, max_len)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x.shape == (batch_size, seq_len, d_model)\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        if seq_len > self.max_len:\n",
    "            raise ValueError(\n",
    "                \"sequence length exceeds model capacity\"\n",
    "            )\n",
    "        \n",
    "        k_t = self.key(x).reshape(batch_size, seq_len, self.num_heads, -1).permute(0, 2, 3, 1)\n",
    "        # k_t.shape = (batch_size, num_heads, d_head, seq_len)\n",
    "        v = self.value(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
    "        q = self.query(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
    "        # shape = (batch_size, num_heads, seq_len, d_head)\n",
    "        \n",
    "        start = self.max_len - seq_len\n",
    "        Er_t = self.Er[start:, :].transpose(0, 1)\n",
    "        # Er_t.shape = (d_head, seq_len)\n",
    "        QEr = torch.matmul(q, Er_t)\n",
    "        # QEr.shape = (batch_size, num_heads, seq_len, seq_len)\n",
    "        Srel = self.skew(QEr)\n",
    "        # Srel.shape = (batch_size, num_heads, seq_len, seq_len)\n",
    "        \n",
    "        QK_t = torch.matmul(q, k_t)\n",
    "        # QK_t.shape = (batch_size, num_heads, seq_len, seq_len)\n",
    "        attn = (QK_t + Srel) / math.sqrt(q.size(-1))\n",
    "        mask = self.mask[:, :, :seq_len, :seq_len]\n",
    "        # mask.shape = (1, 1, seq_len, seq_len)\n",
    "        attn = attn.masked_fill(mask == 0, float(\"-inf\"))\n",
    "        # attn.shape = (batch_size, num_heads, seq_len, seq_len)\n",
    "        attn = f.softmax(attn, dim=-1)\n",
    "        out = torch.matmul(attn, v)\n",
    "        # out.shape = (batch_size, num_heads, seq_len, d_head)\n",
    "        out = out.transpose(1, 2)\n",
    "        # out.shape == (batch_size, seq_len, num_heads, d_head)\n",
    "        out = out.reshape(batch_size, seq_len, -1)\n",
    "        # out.shape == (batch_size, seq_len, d_model)\n",
    "        return self.dropout(out)\n",
    "        \n",
    "    \n",
    "    def skew(self, QEr):\n",
    "        # QEr.shape = (batch_size, num_heads, seq_len, seq_len)\n",
    "        padded = f.pad(QEr, (1, 0))\n",
    "        # padded.shape = (batch_size, num_heads, seq_len, 1 + seq_len)\n",
    "        batch_size, num_heads, num_rows, num_cols = padded.shape\n",
    "        reshaped = padded.reshape(batch_size, num_heads, num_cols, num_rows)\n",
    "        # reshaped.size = (batch_size, num_heads, 1 + seq_len, seq_len)\n",
    "        Srel = reshaped[:, :, 1:, :]\n",
    "        # Srel.shape = (batch_size, num_heads, seq_len, seq_len)\n",
    "        return Srel\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, max_len=256, dropout=0.2):\n",
    "        super().__init__()\n",
    "            \n",
    "        self.heads = nn.ModuleList(\n",
    "            [RelativeGlobalAttention(d_model, num_heads, max_len=128, dropout=0.2) for _ in range(num_heads)]\n",
    "        )\n",
    "        \n",
    "        self.linear = nn.Linear(num_heads * d_model, d_model, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(\n",
    "            torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        )\n",
    "    \n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, max_len=256, dropout=0.2):\n",
    "        super().__init__()\n",
    "                 \n",
    "        self.self_attention = MultiHeadAttention(d_model, num_heads, max_len=128, dropout=0.2)\n",
    "        \n",
    "        self.linear_layer = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model, bias=False),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model, d_model, bias=False),\n",
    "        )\n",
    "                 \n",
    "        self.norm = nn.LayerNorm((seq_len,d_model))\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.norm(self.self_attention(x))\n",
    "        x = self.norm(self.linear_layer(x) + x)\n",
    "                 \n",
    "        return x   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Relative position code taken from : https://github.com/evelinehong/Transformer_Relative_Position_PyTorch/blob/master/relative_position.py\n",
    "# All credits goes to @evelinehong\n",
    "class RelativePosition(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim_hid, max_rel_pos):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dim_hid = dim_hid\n",
    "        self.max_rel_pos = max_rel_pos\n",
    "        self.embeddings_table = nn.Parameter(torch.Tensor(max_rel_pos*2 + 1, self.dim_hid))\n",
    "        nn.init.xavier_uniform_(self.embeddings_table)\n",
    "\n",
    "    def forward(self, length_q, length_k):\n",
    "        \n",
    "        range_vec_q = torch.arange(length_q)\n",
    "        range_vec_k = torch.arange(length_k)\n",
    "        \n",
    "        distance_mat = range_vec_k[None, :] - range_vec_q[:, None]\n",
    "        distance_mat_clipped = torch.clamp(distance_mat, -self.max_rel_pos, self.max_rel_pos)\n",
    "        \n",
    "        final_mat = distance_mat_clipped + self.max_rel_pos\n",
    "        final_mat = torch.LongTensor(final_mat).cuda()\n",
    "        \n",
    "        embeddings = self.embeddings_table[final_mat].cuda()\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, dim_in, dim_hid):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dim_hid = dim_hid\n",
    "        \n",
    "        self.q = nn.Linear(dim_in, self.dim_hid, bias=False)\n",
    "        self.k = nn.Linear(dim_in, self.dim_hid, bias=False)\n",
    "        self.v = nn.Linear(dim_in, self.dim_hid, bias=False)    \n",
    "\n",
    "    def forward(self, query, key, value, rel_pos_k, rel_pos_v, mask=False):\n",
    "\n",
    "        rel_k = rel_pos_k(seq_len, seq_len)\n",
    "        rel_v = rel_pos_v(seq_len, seq_len)\n",
    "        \n",
    "        q = self.q(query)\n",
    "        k = self.k(key)\n",
    "        v = self.v(value)\n",
    "\n",
    "        if mask:\n",
    "            mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).to(query.device)\n",
    "            mask[mask.bool()] = -float('inf')\n",
    "        else:\n",
    "            mask = 0.\n",
    "\n",
    "        # Classic attention\n",
    "        cls_attn = q.bmm(k.transpose(1, 2))\n",
    "        \n",
    "        # Relative attention    \n",
    "        rel_attn = q.permute(1,0,2).bmm(rel_k.permute(0,2,1)).permute(1,0,2)\n",
    "        \n",
    "        attn = (cls_attn + rel_attn) / cls_attn.size(-1) ** 0.5\n",
    "        attn = f.softmax(attn + mask, dim=-1)\n",
    "        \n",
    "        x = attn.bmm(v)\n",
    "        x += attn.permute(1,0,2).matmul(rel_v).permute(1,0,2)\n",
    "        \n",
    "        return x   \n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, dim_in, dim_hid):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.max_rel_pos = seq_len   \n",
    "        \n",
    "        self.rel_pos_k = RelativePosition(dim_hid, self.max_rel_pos)\n",
    "        self.rel_pos_v = RelativePosition(dim_hid, self.max_rel_pos) \n",
    "        \n",
    "        self.heads = nn.ModuleList(\n",
    "            [AttentionHead(dim_in, dim_hid) for _ in range(num_heads)]\n",
    "        )\n",
    "        self.linear = nn.Linear(num_heads * dim_hid, dim_in, bias=False)\n",
    "\n",
    "    def forward(self, query, key, value, mask=0.):\n",
    "        \n",
    "        return self.linear(\n",
    "            torch.cat([h(query, key, value, self.rel_pos_k, self.rel_pos_v, mask) for h in self.heads], dim=-1)\n",
    "        )\n",
    "    \n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, num_heads=32, dim_in=64, dim_hid=32, dim_ff=128, dropout=0.2):\n",
    "        super().__init__()\n",
    "                 \n",
    "        self.dim_in = dim_in\n",
    "        self.dim_ff = dim_ff\n",
    "        self.dim_hid = dim_hid\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.self_attention = MultiHeadAttention(num_heads=self.num_heads, dim_in=self.dim_in, dim_hid=self.dim_hid)\n",
    "        \n",
    "        self.linear_layer = nn.Sequential(\n",
    "            nn.Linear(self.dim_in, self.dim_ff, bias=False),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(self.dim_ff, self.dim_in, bias=False),\n",
    "        )\n",
    "                 \n",
    "        self.norm = nn.LayerNorm((seq_len,self.dim_in))\n",
    "        \n",
    "    def forward(self, x, mask=0.):\n",
    "\n",
    "        x = self.norm(self.self_attention(x,x,x, mask) + x)\n",
    "        x = self.norm(self.linear_layer(x) + x)\n",
    "                 \n",
    "        return x \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MidiAI(nn.Module):\n",
    "    def __init__(self, num_layers=14, num_heads=12, embedding_size=64, dim_hid=32, dim_ff=128, dropout=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.dim_hid = dim_hid\n",
    "        self.dim_ff = dim_ff\n",
    "        \n",
    "        self.embedding_size = embedding_size\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.embedding = nn.Embedding(129,self.embedding_size-3)\n",
    "        \n",
    "        layers = []\n",
    "        for _ in range(self.num_layers):\n",
    "            layer = TransformerEncoder(num_heads=self.num_heads, dim_in=self.embedding_size, dim_hid=self.dim_hid, dim_ff=self.dim_ff, dropout=self.dropout)\n",
    "            layer = nn.DataParallel(layer, device_ids=device_ids)\n",
    "            layers.append(layer)\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "                \n",
    "        self.linear = nn.Linear(self.embedding_size,131)\n",
    "        self.norm = nn.LayerNorm((seq_len,self.embedding_size))\n",
    "        \n",
    "        self.embedding = nn.DataParallel(self.embedding, device_ids=device_ids)\n",
    "        self.linear = nn.DataParallel(self.linear, device_ids=device_ids)\n",
    "        self.norm = nn.DataParallel(self.norm, device_ids=device_ids)             \n",
    "    \n",
    "    def prepare_input(self, x_in):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x = x_in.clone()\n",
    "            x[x[:,:,2] < 128] = 0.\n",
    "            \n",
    "            time_stamps = torch.cumsum(x[:,:,0], dim=1)\n",
    "            max_value = time_stamps[:,-1:].clone()\n",
    "            time_stamps /= max_value\n",
    "\n",
    "        x = torch.zeros(x_in.shape[0], x_in.shape[1], self.embedding_size-1).to(x_in.device)\n",
    "        x[:,:,0:3] = x_in.clone()\n",
    "        x = torch.cat([time_stamps.unsqueeze(-1), x], dim=2)\n",
    "        \n",
    "        x[:,:,3:] = self.embedding(x[:,:,3].long()) * math.sqrt(self.embedding_size)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def forward(self, x_in):\n",
    "\n",
    "        x = self.prepare_input(x_in)\n",
    "\n",
    "        for layer in self.layers:\n",
    "             x = layer(x, mask=True)\n",
    "        \n",
    "        x = self.linear(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = MidiAI().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters = 2,098,048\n"
     ]
    }
   ],
   "source": [
    "print(\"Parameters = {:,.0f}\".format(count_parameters(m)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(m.parameters(), lr=lr)\n",
    "#scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1e-3, epochs=n_epochs, steps_per_epoch=len(data_loader)//mini_batch_cycles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bce_loss = nn.BCEWithLogitsLoss()\n",
    "mse_loss = nn.MSELoss()\n",
    "crossentropy_loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_composition(model, x_in, extension_length, max_length):\n",
    "    with torch.no_grad():\n",
    "        context = None\n",
    "\n",
    "        x_out = x_in.clone()\n",
    "        for i in range(extension_length):\n",
    "            y = model(x_out[:,i:i+x_in.shape[1],:])\n",
    "            y[:,:,2] = y[:,:,2:].argmax(2)\n",
    "            x_out = torch.cat([x_out,y[:,-1:,:3].clone()], dim=1)\n",
    "\n",
    "        x_out[:,:,0] = x_out[:,:,0] * max_length\n",
    "        x_out[:,:,1] = x_out[:,:,1] * 128\n",
    "\n",
    "    return x_out[:,:,:3]\n",
    "\n",
    "def swap_on_condition(t, fct, rdn_trsh = 0.5):\n",
    "    t = t.numpy()\n",
    "    n = t.shape[1]\n",
    "    for k in range(t.shape[0]):\n",
    "        for i in range(n-1):\n",
    "            if fct(t[k,:,:],i) and (random.random()<rdn_trsh):\n",
    "                t[k,i,:], t[k,i+1,:] = t[k,i+1,:], t[k,i,:].copy()\n",
    "    return torch.tensor(t)\n",
    "\n",
    "def add_delta(x, delta):\n",
    "    \n",
    "    x += torch.Tensor([delta]).to(x.device)\n",
    "\n",
    "    floor = torch.FloatTensor([0]).to(x.device)\n",
    "    ceil = torch.FloatTensor([127]).to(x.device)\n",
    "    x = torch.max(x, floor.expand_as(x))\n",
    "    x = torch.min(x, ceil.expand_as(x))\n",
    "    \n",
    "    return x\n",
    "\n",
    "def prepare_batch(x, velocity_delta=0, note_delta=0, noise=False):\n",
    "    with torch.no_grad():\n",
    "        max_length = x[:,:,0].max(1)[0]\n",
    "        max_length = torch.mul(max_length, stretch).reshape(-1,1)\n",
    "\n",
    "        v_delta = random.randint(-velocity_delta, velocity_delta) \n",
    "        n_delta = random.randint(-note_delta, note_delta)\n",
    "\n",
    "        if noise:\n",
    "            x[:,:,0] += torch.randint(-insignificant_timestep, insignificant_timestep, size=x[:,:,0].shape)  #length noise\n",
    "            x[:,:,1] += torch.randint(-insignificant_velocity, insignificant_velocity, size=x[:,:,1].shape)  #velocity noise\n",
    "\n",
    "        x[:,:,1] = add_delta(x[:,:,1], v_delta) #velocity delta\n",
    "        x[:,:,2][x[:,:,2]<128] = add_delta(x[:,:,2][x[:,:,2]<128], n_delta) #note delta\n",
    "\n",
    "        x[:,:,0] /= max_length #length\n",
    "        x[:,:,1] /= 127 #velocity\n",
    "\n",
    "        #Randomly swap notes if happening at the sime time (not a 'wait')\n",
    "        x = swap_on_condition(x, lambda x,i : ((x[i,2] != 128) and (x[i+1,2] != 128) and (i+1<=x.shape[0])), rdn_trsh=0.5)\n",
    "\n",
    "    return x, max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 | 1000 | Losses | velocity = 6.31e-02 | length = 3.88e-02 | note = 2.72e+00 | all = 9.40e-01\n",
      "1 | 2000 | Losses | velocity = 4.93e-02 | length = 3.48e-02 | note = 2.26e+00 | all = 7.80e-01\n",
      "1 | 3000 | Losses | velocity = 4.45e-02 | length = 3.42e-02 | note = 2.07e+00 | all = 7.16e-01\n",
      "1 | 4000 | Losses | velocity = 4.30e-02 | length = 3.38e-02 | note = 1.95e+00 | all = 6.76e-01\n",
      "1 | 5000 | Losses | velocity = 4.11e-02 | length = 3.36e-02 | note = 1.84e+00 | all = 6.40e-01\n",
      "1 | 6000 | Losses | velocity = 3.94e-02 | length = 3.34e-02 | note = 1.74e+00 | all = 6.04e-01\n",
      "1 | 7000 | Losses | velocity = 3.85e-02 | length = 3.31e-02 | note = 1.65e+00 | all = 5.74e-01\n",
      "1 | 8000 | Losses | velocity = 3.76e-02 | length = 3.29e-02 | note = 1.57e+00 | all = 5.48e-01\n",
      "1 | 9000 | Losses | velocity = 3.68e-02 | length = 3.26e-02 | note = 1.51e+00 | all = 5.28e-01\n",
      "1 | 10000 | Losses | velocity = 3.56e-02 | length = 3.20e-02 | note = 1.45e+00 | all = 5.07e-01\n",
      "1 | 11000 | Losses | velocity = 3.43e-02 | length = 3.14e-02 | note = 1.41e+00 | all = 4.91e-01\n",
      "1 | 12000 | Losses | velocity = 3.29e-02 | length = 3.09e-02 | note = 1.37e+00 | all = 4.78e-01\n",
      "1 | 13000 | Losses | velocity = 3.14e-02 | length = 3.05e-02 | note = 1.34e+00 | all = 4.68e-01\n",
      "1 | 14000 | Losses | velocity = 3.08e-02 | length = 3.00e-02 | note = 1.32e+00 | all = 4.59e-01\n",
      "1 | 15000 | Losses | velocity = 3.00e-02 | length = 2.93e-02 | note = 1.29e+00 | all = 4.50e-01\n",
      "1 | 16000 | Losses | velocity = 2.92e-02 | length = 2.88e-02 | note = 1.27e+00 | all = 4.41e-01\n",
      "1 | 17000 | Losses | velocity = 2.84e-02 | length = 2.85e-02 | note = 1.24e+00 | all = 4.34e-01\n",
      "1 | 18000 | Losses | velocity = 2.82e-02 | length = 2.82e-02 | note = 1.22e+00 | all = 4.27e-01\n",
      "1 | 19000 | Losses | velocity = 2.79e-02 | length = 2.81e-02 | note = 1.21e+00 | all = 4.21e-01\n",
      "1 | 20000 | Losses | velocity = 2.71e-02 | length = 2.78e-02 | note = 1.19e+00 | all = 4.14e-01\n",
      "1 | 21000 | Losses | velocity = 2.70e-02 | length = 2.77e-02 | note = 1.17e+00 | all = 4.09e-01\n",
      "1 | 22000 | Losses | velocity = 2.68e-02 | length = 2.75e-02 | note = 1.16e+00 | all = 4.03e-01\n",
      "1 | 23000 | Losses | velocity = 2.66e-02 | length = 2.74e-02 | note = 1.14e+00 | all = 3.98e-01\n",
      "1 | 24000 | Losses | velocity = 2.62e-02 | length = 2.73e-02 | note = 1.12e+00 | all = 3.92e-01\n",
      "1 | 25000 | Losses | velocity = 2.60e-02 | length = 2.72e-02 | note = 1.11e+00 | all = 3.88e-01\n",
      "1 | 26000 | Losses | velocity = 2.59e-02 | length = 2.71e-02 | note = 1.10e+00 | all = 3.85e-01\n",
      "1 | 27000 | Losses | velocity = 2.55e-02 | length = 2.70e-02 | note = 1.09e+00 | all = 3.82e-01\n",
      "1 | 28000 | Losses | velocity = 2.56e-02 | length = 2.70e-02 | note = 1.08e+00 | all = 3.78e-01\n",
      "1 | 29000 | Losses | velocity = 2.54e-02 | length = 2.69e-02 | note = 1.07e+00 | all = 3.74e-01\n",
      "1 | 30000 | Losses | velocity = 2.52e-02 | length = 2.68e-02 | note = 1.06e+00 | all = 3.71e-01\n",
      "1 | 31000 | Losses | velocity = 2.51e-02 | length = 2.68e-02 | note = 1.06e+00 | all = 3.69e-01\n",
      "1 | 32000 | Losses | velocity = 2.51e-02 | length = 2.67e-02 | note = 1.05e+00 | all = 3.66e-01\n",
      "1 | 33000 | Losses | velocity = 2.50e-02 | length = 2.67e-02 | note = 1.04e+00 | all = 3.63e-01\n",
      "1 | 34000 | Losses | velocity = 2.46e-02 | length = 2.66e-02 | note = 1.03e+00 | all = 3.60e-01\n",
      "1 | 35000 | Losses | velocity = 2.47e-02 | length = 2.66e-02 | note = 1.02e+00 | all = 3.58e-01\n",
      "1 | 36000 | Losses | velocity = 2.45e-02 | length = 2.65e-02 | note = 1.02e+00 | all = 3.56e-01\n",
      "1 | 37000 | Losses | velocity = 2.43e-02 | length = 2.65e-02 | note = 1.01e+00 | all = 3.52e-01\n",
      "1 | 38000 | Losses | velocity = 2.42e-02 | length = 2.64e-02 | note = 9.99e-01 | all = 3.50e-01\n"
     ]
    }
   ],
   "source": [
    "for i_epoch in range(1, n_epochs+1):\n",
    "    \n",
    "    velocity_losses = []\n",
    "    length_losses = []\n",
    "    note_losses = []\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for i_batch, data in enumerate(data_loader,1):\n",
    "            \n",
    "        m.train()\n",
    "        seq = data[0]\n",
    "        \n",
    "        seq,max_length = prepare_batch(seq, velocity_delta=20, note_delta=12, noise=True)\n",
    "        \n",
    "        x = seq[:,:-1,:].to(device)\n",
    "        target = seq[:,1:,:].to(device)\n",
    "\n",
    "        y = m(x)\n",
    "\n",
    "        y_length = y[:,:,0]\n",
    "        y_velocity = y[:,:,1]\n",
    "        y_note = y[:,:,2:].permute(0,2,1)\n",
    "        \n",
    "        target_length = target[:,:,0]\n",
    "        target_velocity = target[:,:,1]\n",
    "        target_note = target[:,:,2]\n",
    "        \n",
    "        length_loss = mse_loss(y_length, target_length)\n",
    "        velocity_loss = mse_loss(y_velocity, target_velocity)\n",
    "        note_loss = crossentropy_loss(y_note, target_note.long())\n",
    "        \n",
    "        loss = length_loss + velocity_loss + note_loss\n",
    "        \n",
    "        velocity_losses.append(velocity_loss.cpu().detach())\n",
    "        length_losses.append(length_loss.cpu().detach())\n",
    "        note_losses.append(note_loss.cpu().detach())\n",
    "        \n",
    "        loss = length_loss + velocity_loss + note_loss\n",
    "        loss.backward()\n",
    "        \n",
    "        if i_batch % mini_batch_cycles == 0:\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(m.parameters(), max_norm=8, norm_type=2.0)\n",
    "            optimizer.step()\n",
    "            #scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            if i_batch % batch_display_freq == 0:\n",
    "                print(\"{} | {} | Losses | velocity = {:.2e} | length = {:.2e} | note = {:.2e} | all = {:.2e}\".format(\n",
    "                    i_epoch,\n",
    "                    i_batch,\n",
    "                    np.array(velocity_losses).mean(),\n",
    "                    np.array(length_losses).mean(),\n",
    "                    np.array(note_losses).mean(),\n",
    "                    np.array(length_losses+note_losses+velocity_losses).mean()\n",
    "                ))    \n",
    "\n",
    "                with open('{}_log.txt'.format(model_tag), 'a') as log:\n",
    "                    log.write(\"{} | {} | lr = {:.2e} | velocity = {:.2e} | length = {:.2e} | note = {:.2e} | all = {:.2e}\\n\".format(\n",
    "                    i_epoch,\n",
    "                    i_batch,\n",
    "                    lr,\n",
    "                    np.array(velocity_losses).mean(),\n",
    "                    np.array(length_losses).mean(),\n",
    "                    np.array(note_losses).mean(),\n",
    "                    np.array(length_losses+note_losses+velocity_losses).mean()\n",
    "                ))\n",
    "\n",
    "                velocity_losses = []\n",
    "                length_losses = []\n",
    "                note_losses = []\n",
    "\n",
    "            if i_batch % save_audio_freq == 0:      \n",
    "\n",
    "                write_folder = os.path.join('.',output_folder,'{}_e{}_b{}'.format(model_tag,i_epoch,i_batch))\n",
    "                if not os.path.exists(write_folder): os.makedirs(write_folder) \n",
    "\n",
    "                data = next(iter(test_data_loader))\n",
    "                seq = data[0]\n",
    "\n",
    "                # Normalize\n",
    "                seq,max_length = prepare_batch(seq)\n",
    "                x = seq[:,:-1,:].to(device)\n",
    "                m = m.to(device)\n",
    "                max_length = max_length.to(device)\n",
    "\n",
    "                x_ext = extend_composition(m, x, extension_length=seq_len, max_length=max_length)\n",
    "\n",
    "                for i in range(x_ext.shape[0]):\n",
    "                    track = decode_track(x_ext[i,:,:].float().detach().cpu().numpy())\n",
    "                    mid_out = mido.MidiFile(type=0)\n",
    "                    mid_out.tracks.append(track)\n",
    "\n",
    "                    file_path = os.path.join(write_folder, './{}_TestOutput_e{}_b{}_{}.mid'.format(model_tag,i_epoch,i_batch,i)) \n",
    "\n",
    "                    write_midi(mid_out, file_path)\n",
    "\n",
    "                write_folder = os.path.join('.',output_folder,'{}_e{}_b{}'.format(model_tag,i_epoch,i_batch,i))\n",
    "                if not os.path.exists(write_folder): os.makedirs(write_folder)\n",
    "\n",
    "                file_path = os.path.join(write_folder,'{}_model_e{}_b{}_{}.pt'.format(model_tag, i_epoch, i_batch, i))\n",
    "\n",
    "                torch.save(m.state_dict(), file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
